{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Dataset API\n",
    "\n",
    "Within this notebook I aim to get a better understanding of the workings of tf dataset vs using feed-dict. Dataset would ensure that the GPU would never have to wait for new stuff to come in.\n",
    "\n",
    "Generally, need to follow 3 steps:\n",
    "1. **Import the data**: create dataset instance for some data\n",
    "2. **Create an iterator**: make an iterator form the creater dataset\n",
    "3. **Consuming the dataset**: from iterator we get the data that we need for our models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nina/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data\n",
    "\n",
    "Below will be ways to import data into tensorflow form different locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numpy**\n",
    "\n",
    "Can pass both a single or multiple numpy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##single vector (random)\n",
    "x = np.random.sample((50,2))\n",
    "\n",
    "#make dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (2,), types: tf.float64>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04755973 0.46445782]\n"
     ]
    }
   ],
   "source": [
    "it = dataset.make_one_shot_iterator()\n",
    "el = it.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple numpy vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.00995442, 0.8160845 ]), array([0.37720206]))\n"
     ]
    }
   ],
   "source": [
    "it = dataset.make_one_shot_iterator()\n",
    "el = it.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tf.random_uniform([100,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2864604 0.7926196]\n"
     ]
    }
   ],
   "source": [
    "it = dataset.make_initializable_iterator()\n",
    "el = it.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #need to initialize iterator\n",
    "    sess.run(it.initializer) #important!!!!!\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can note that have to run the initializer!!! **ALWAYS REMEMEBR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Placeholder**\n",
    "\n",
    "Useful when you want to dynamically change data inside a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95630896 0.69142455]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "data = np.random.sample((100,2))\n",
    "\n",
    "it = dataset.make_initializable_iterator()\n",
    "el = it.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(it.initializer, feed_dict={x: data})\n",
    "    print(sess.run(el))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generator**\n",
    "\n",
    "Useful when we have array of different elements length - like a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
