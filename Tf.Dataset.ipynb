{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Dataset API\n",
    "\n",
    "Within this notebook I aim to get a better understanding of the workings of tf dataset vs using feed-dict. Dataset would ensure that the GPU would never have to wait for new stuff to come in.\n",
    "\n",
    "Generally, need to follow 3 steps:\n",
    "1. **Import the data**: create dataset instance for some data\n",
    "2. **Create an iterator**: make an iterator form the creater dataset\n",
    "3. **Consuming the dataset**: from iterator we get the data that we need for our models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nina/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data\n",
    "\n",
    "Below will be ways to import data into tensorflow form different locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numpy**\n",
    "\n",
    "Can pass both a single or multiple numpy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##single vector (random)\n",
    "x = np.random.sample((50,2))\n",
    "\n",
    "#make dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (2,), types: tf.float64>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04755973 0.46445782]\n"
     ]
    }
   ],
   "source": [
    "it = dataset.make_one_shot_iterator()\n",
    "el = it.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple numpy vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.00995442, 0.8160845 ]), array([0.37720206]))\n"
     ]
    }
   ],
   "source": [
    "it = dataset.make_one_shot_iterator()\n",
    "el = it.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tf.random_uniform([100,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2864604 0.7926196]\n"
     ]
    }
   ],
   "source": [
    "it = dataset.make_initializable_iterator()\n",
    "el = it.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #need to initialize iterator\n",
    "    sess.run(it.initializer) #important!!!!!\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can note that have to run the initializer!!! **ALWAYS REMEMEBR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Placeholder**\n",
    "\n",
    "Useful when you want to dynamically change data inside a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95630896 0.69142455]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "data = np.random.sample((100,2))\n",
    "\n",
    "it = dataset.make_initializable_iterator()\n",
    "el = it.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(it.initializer, feed_dict={x: data})\n",
    "    print(sess.run(el))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generator**\n",
    "\n",
    "Useful when we have array of different elements length - like a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[[2]\n",
      " [3]]\n",
      "[[3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "sequence = np.array([[[1]], [[2],[3]], [[3],[4],[5]]])\n",
    "\n",
    "#create generator\n",
    "def generator():\n",
    "    for el in sequence:\n",
    "        yield el\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(generator,\n",
    "                                           output_types= tf.int64, output_shapes=(tf.TensorShape([None, 1])))\n",
    "it = dataset.make_initializable_iterator()\n",
    "el = it.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(it.initializer)\n",
    "    print(sess.run(el))\n",
    "    print(sess.run(el))\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different types of iterators that can be used which will allow us to retrieve values of our data.\n",
    "\n",
    "* **One Shot**: iterate once through the dataset and cannot feed anything to it\n",
    "* **Initalizable**: can dynamically feed in data with feed_dict\n",
    "* **Reinitializable**: can be initialized from different Dataset. Useful when training dataset may undergo transformations\n",
    "* **Feedable**: used to select which iterator to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 2.], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.array([[1,2]]), np.array([[0]]))\n",
    "\n",
    "it = dataset.make_initializable_iterator()\n",
    "feats, labels = it.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(it.initializer, feed_dict={x:train_data[0], y:train_data[1]})\n",
    "    for i in range(EPOCHS):\n",
    "        sess.run([feats, labels])\n",
    "    #switch to test\n",
    "    sess.run(it.initializer, feed_dict={x:test_data[0], y:test_data[1]})\n",
    "    print(sess.run([feats, labels]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinitializable**\n",
    "\n",
    "We are switching *between DATASETS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.96644433, 0.79570336]), array([0.74923463])]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "\n",
    "#train_test data\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "\n",
    "#create datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data)\n",
    "\n",
    "#!!!!CREATE GENERIC ITERATOR!!!!\n",
    "it = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "feats, labels = it.get_next()\n",
    "#crete initialisation operations\n",
    "train_init_op = it.make_initializer(train_dataset)\n",
    "test_init_op = it.make_initializer(test_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(train_init_op) #run train initializer\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([feats, labels])\n",
    "    #switch to test\n",
    "    sess.run(test_init_op)\n",
    "    print(sess.run([feats,labels]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feedable**\n",
    "\n",
    "We are switching *between ITERATORS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "#my data\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "\n",
    "#pplaceholders\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "#create datsets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "\n",
    "#we could also have had one shot ( in that case no placeholders needed)\n",
    "train_it = train_dataset.make_initializable_iterator()\n",
    "test_it = test_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*handle*: placeholder which can be dynamically changed - allows us to switch between the different iterators which have been defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.48809168, 0.5892124 ], dtype=float32), array([0.71873456], dtype=float32))\n",
      "(array([0.39999062, 0.8828608 ], dtype=float32), array([0.2170473], dtype=float32))\n",
      "(array([0.92552376, 0.33964977], dtype=float32), array([0.75877225], dtype=float32))\n",
      "(array([0.33813456, 0.25078583], dtype=float32), array([0.04937245], dtype=float32))\n",
      "(array([0.945379  , 0.38982236], dtype=float32), array([0.32936984], dtype=float32))\n",
      "(array([0.9840039 , 0.91464275], dtype=float32), array([0.61121887], dtype=float32))\n",
      "(array([0.4283472 , 0.05491492], dtype=float32), array([0.95066553], dtype=float32))\n",
      "(array([0.9981808 , 0.26911464], dtype=float32), array([0.1444552], dtype=float32))\n",
      "(array([0.28101063, 0.5318586 ], dtype=float32), array([0.41510203], dtype=float32))\n",
      "(array([0.0916037, 0.4734865], dtype=float32), array([0.86678237], dtype=float32))\n",
      "(array([0.9377489, 0.9165799], dtype=float32), array([0.9074531], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "\n",
    "#create generic iterator - this will allow us to switch between iterators\n",
    "#previously the generic iterator allowed us to switch between datasets\n",
    "it = tf.data.Iterator.from_string_handle(handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "els = it.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_handle = sess.run(train_it.string_handle())\n",
    "    test_handle = sess.run(test_it.string_handle())\n",
    "    \n",
    "    #initialize the different iterators\n",
    "    sess.run(train_it.initializer, feed_dict={x: train_data[0], y:train_data[1]})\n",
    "    sess.run(test_it.initializer, feed_dict={x: test_data[0], y: test_data[1]})\n",
    "    \n",
    "    for _ in range(EPOCHS):\n",
    "        x,y = sess.run(els, feed_dict={handle: train_handle})\n",
    "        print(x,y)\n",
    "    x,y = sess.run(els, feed_dict={handle: test_handle})\n",
    "    print(x,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consuming the data**\n",
    "\n",
    "In the following code certain new elements are used.\n",
    "* ` batch ` : batches data with provided size\n",
    "* `repeat`: specifies number of times dataset has to be repeated. If no argument is passed it will run forever. This is ideal as it can then be tweaked by the number of epochs we are choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.2334\n",
      "Iter: 1, Loss: 0.2265\n",
      "Iter: 2, Loss: 0.2199\n",
      "Iter: 3, Loss: 0.2134\n",
      "Iter: 4, Loss: 0.2071\n",
      "Iter: 5, Loss: 0.2010\n",
      "Iter: 6, Loss: 0.1952\n",
      "Iter: 7, Loss: 0.1895\n",
      "Iter: 8, Loss: 0.1841\n",
      "Iter: 9, Loss: 0.1788\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "# using two numpy arrays\n",
    "features, labels = (np.array([np.random.sample((100,2))]), \n",
    "                    np.array([np.random.sample((100,1))])) #these are wrapped in another array b/c needed for batching\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE) \n",
    "iter = dataset.make_one_shot_iterator() #nothing is feeded inside\n",
    "x, y = iter.get_next()\n",
    "\n",
    "# make a simple model - v small nn\n",
    "net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(EPOCHS):\n",
    "        _, loss_value = sess.run([train_op, loss])\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
